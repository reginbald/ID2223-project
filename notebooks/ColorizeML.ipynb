{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLab(image, label):\n",
    "    lab = color.rgb2lab(label)\n",
    "    X_batch = lab[:,:,0]\n",
    "    Y_batch = lab[:,:,1:]\n",
    "    return X_batch.reshape(X_batch.shape+(1,)), Y_batch\n",
    "\n",
    "\n",
    "def _parse_function(filename):\n",
    "    image = tf.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize_images(image, [256, 256], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    label = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.divide(label, 255.0)\n",
    "    \n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "file_paths = map((lambda x: \"../data/\" + x), os.listdir(\"../data/\"))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.map(lambda image, label: \n",
    "    tuple(tf.py_func(\n",
    "        convertToLab, [image, label], [tf.double, tf.double]\n",
    "    ))\n",
    ")\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_paths = map((lambda x: \"../data/\" + x), os.listdir(\"../data/\"))\n",
    "#\n",
    "#all_images = ops.convert_to_tensor(file_paths, dtype=dtypes.string)\n",
    "#\n",
    "#train_input_queue = tf.train.slice_input_producer(\n",
    "#                                    [all_images, all_images],\n",
    "#                                    shuffle=False)\n",
    "#\n",
    "#\n",
    "#file_content = tf.read_file(train_input_queue[0])\n",
    "#train_image = tf.image.decode_jpeg(file_content, channels=3)\n",
    "#train_image = tf.image.resize_images(train_image, [256, 256], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#train_image = tf.image.rgb_to_grayscale(train_image)\n",
    "#train_image = tf.image.convert_image_dtype(train_image, tf.float32)\n",
    "#\n",
    "#label_content = tf.read_file(train_input_queue[1])\n",
    "#train_label = tf.image.decode_jpeg(file_content, channels=3)\n",
    "#train_label = tf.image.resize_images(train_label, [256, 256], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#train_label = tf.image.convert_image_dtype(train_label, tf.float32)\n",
    "#train_label = tf.divide(train_label, 255.0)\n",
    "#\n",
    "#\n",
    "#                    \n",
    "## collect batches of images before processing\n",
    "#train_image_batch, train_label_batch = tf.train.batch(\n",
    "#    [train_image, train_label],\n",
    "#    batch_size=batch_size\n",
    "#    #,num_threads=1\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(num, imgs):\n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(num): #length of your filename list\n",
    "        image = imgs.eval() \n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareImages(imgs):\n",
    "    sz_imgs = tf.image.resize_images(imgs, [256, 256], tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    bw_imgs = tf.image.rgb_to_grayscale(sz_imgs)\n",
    "    \n",
    "    return sz_imgs, bw_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageLabGenerator(size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=size):\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2DRelu(X, W, B, strides, padding):\n",
    "    # strides: [batch_step, height_step, width_step, channel_step] \n",
    "    return tf.nn.relu(tf.nn.conv2d(X, W, strides=strides, padding=padding) + B)\n",
    "\n",
    "def conv2DTanh(X, W, B, strides, padding):\n",
    "    # strides: [batch_step, height_step, width_step, channel_step] \n",
    "    return tf.nn.tanh(tf.nn.conv2d(X, W, strides=strides, padding=padding) + B)\n",
    "\n",
    "def weight(width, height, input_channels, output_channels):\n",
    "    # [width, height, input channel, output channel]\n",
    "    return tf.Variable(tf.truncated_normal([width, height, input_channels, output_channels], stddev=0.1))\n",
    "\n",
    "def bias(outputChannels):\n",
    "    return tf.Variable(tf.zeros([outputChannels])) # bias for each output channel.\n",
    "\n",
    "def upSampling2D(X, height, width):\n",
    "    return tf.image.resize_images(X, [height, width], tf.image.ResizeMethod.NEAREST_NEIGHBOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input images x will consist of a 2d tensor of floating point numbers.\n",
    "#X = tf.placeholder(tf.float32, shape=[None, [256, 256, 1]]) # 256 x 256\n",
    "# The target output y_ will also consist of a 2d tensor, where each row is \n",
    "# a one-hot 2-dimensional vector indicating a b values.\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 256, 256, 2])\n",
    "\n",
    "# Input Layer\n",
    "X = tf.placeholder(tf.float32, shape=[None, 256, 256, 1]) # 256 x 256\n",
    "\n",
    "# Conv layer 1 \n",
    "W1 = weight(5, 5, 1, 64)\n",
    "B1 = bias(64)\n",
    "Y1 = conv2DRelu(X, W1, B1, [1,1,1,1], 'SAME')\n",
    "\n",
    "# Conv layer 2\n",
    "W2 = weight(5, 5, 64, 64)\n",
    "B2 = bias(64)\n",
    "Y2 = conv2DRelu(Y1, W2, B2, [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "# Conv layer 3\n",
    "W3 = weight(5, 5, 64, 128)\n",
    "B3 = bias(128)\n",
    "Y3 = conv2DRelu(Y2, W3, B3, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Conv layer 4\n",
    "W4 = weight(5, 5, 128, 128)\n",
    "B4 = bias(128)\n",
    "Y4 = conv2DRelu(Y3, W4, B4, [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "# Conv layer 5\n",
    "W5 = weight(5, 5, 128, 256)\n",
    "B5 = bias(256)\n",
    "Y5 = conv2DRelu(Y4, W5, B5, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Conv layer 6\n",
    "W6 = weight(5, 5, 256, 256)\n",
    "B6 = bias(256)\n",
    "Y6 = conv2DRelu(Y5, W6, B6, [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "# Conv layer 7\n",
    "W7 = weight(5, 5, 256, 512)\n",
    "B7 = bias(512)\n",
    "Y7 = conv2DRelu(Y6, W7, B7, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Conv layer 8\n",
    "W8 = weight(5, 5, 512, 256)\n",
    "B8 = bias(256)\n",
    "Y8 = conv2DRelu(Y7, W8, B8, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Conv layer 9\n",
    "W9 = weight(5, 5, 256, 128)\n",
    "B9 = bias(128)\n",
    "Y9 = conv2DRelu(Y8, W9, B9, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Up sampling layer 1\n",
    "Y10 = upSampling2D(Y9, 64, 64)\n",
    "\n",
    "# Conv layer 10\n",
    "W11 = weight(5, 5, 128, 64)\n",
    "B11 = bias(64)\n",
    "Y11 = conv2DRelu(Y10, W11, B11, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Up sampling layer 2\n",
    "Y12 = upSampling2D(Y11, 128, 128)\n",
    "\n",
    "# Conv layer 11\n",
    "W13 = weight(5, 5, 64, 32)\n",
    "B13 = bias(32)\n",
    "Y13 = conv2DRelu(Y12, W13, B13, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Conv layer 12\n",
    "W14 = weight(5, 5, 32, 2)\n",
    "B14 = bias(2)\n",
    "Y14 = conv2DTanh(Y13, W14, B14, [1, 1, 1, 1], 'SAME')\n",
    "\n",
    "# Up sampling layer 3\n",
    "Y15 = upSampling2D(Y14, 256, 256)\n",
    "\n",
    "# Define the loss function \n",
    "loss = tf.reduce_mean(tf.squared_difference(Y15, Y_)) \n",
    "\n",
    "# Define an optimizer\n",
    "train_step = tf.train.AdamOptimizer(0.005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(i, batch_X, batch_Y):\n",
    "    print batch_X\n",
    "    print batch_Y\n",
    "    print \"\\r\", i,\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the train set:\n",
      "[[[[ 0.2073524 ]\n",
      "   [ 0.20888492]\n",
      "   [ 0.2110353 ]\n",
      "   ..., \n",
      "   [ 0.19899313]\n",
      "   [ 0.19968859]\n",
      "   [ 0.19822832]]\n",
      "\n",
      "  [[ 0.20958038]\n",
      "   [ 0.21119584]\n",
      "   [ 0.2110353 ]\n",
      "   ..., \n",
      "   [ 0.19984501]\n",
      "   [ 0.19914832]\n",
      "   [ 0.19807313]]\n",
      "\n",
      "  [[ 0.20888492]\n",
      "   [ 0.21088011]\n",
      "   [ 0.21088011]\n",
      "   ..., \n",
      "   [ 0.2000002 ]\n",
      "   [ 0.20183897]\n",
      "   [ 0.19883671]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.06360747]\n",
      "   [ 0.06445936]\n",
      "   [ 0.06569096]\n",
      "   ..., \n",
      "   [ 0.06698824]\n",
      "   [ 0.06622466]\n",
      "   [ 0.06622466]]\n",
      "\n",
      "  [[ 0.06384561]\n",
      "   [ 0.06277042]\n",
      "   [ 0.06606946]\n",
      "   ..., \n",
      "   [ 0.07052542]\n",
      "   [ 0.06622466]\n",
      "   [ 0.06407427]]\n",
      "\n",
      "  [[ 0.06714465]\n",
      "   [ 0.06830156]\n",
      "   [ 0.06499304]\n",
      "   ..., \n",
      "   [ 0.06729985]\n",
      "   [ 0.06453572]\n",
      "   [ 0.06407427]]]]\n",
      "[[[[-0.01100015  0.00786799]\n",
      "   [-0.00391246 -0.01142263]\n",
      "   [-0.01340233 -0.00135665]\n",
      "   ..., \n",
      "   [-0.00616937 -0.00213678]\n",
      "   [-0.00616924 -0.00213704]\n",
      "   [-0.00616883 -0.00213782]]\n",
      "\n",
      "  [[-0.0065713  -0.00733658]\n",
      "   [-0.01824293  0.00866676]\n",
      "   [-0.01462229  0.0082688 ]\n",
      "   ..., \n",
      "   [-0.00834977  0.00379798]\n",
      "   [-0.00616917 -0.00213717]\n",
      "   [-0.01456782  0.0049595 ]]\n",
      "\n",
      "  [[-0.01715129  0.00569666]\n",
      "   [-0.01455464  0.00493453]\n",
      "   [-0.01455444  0.00493414]\n",
      "   ..., \n",
      "   [-0.00725971  0.00083086]\n",
      "   [-0.01088062  0.00122934]\n",
      "   [-0.01347654  0.00199004]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.01455376  0.00493285]\n",
      "   [-0.01381463 -0.00653678]\n",
      "   [ 0.00259208 -0.00075347]\n",
      "   ..., \n",
      "   [-0.01080328 -0.00212331]\n",
      "   [-0.01230844 -0.00433101]\n",
      "   [-0.01436003 -0.0050525 ]]\n",
      "\n",
      "  [[-0.01346397  0.00196624]\n",
      "   [-0.01019276 -0.00693707]\n",
      "   [-0.01628145 -0.01243283]\n",
      "   ..., \n",
      "   [-0.01019276 -0.00693707]\n",
      "   [-0.01230885 -0.00433024]\n",
      "   [-0.01339959 -0.00136182]]\n",
      "\n",
      "  [[-0.01715123  0.00569653]\n",
      "   [-0.00971357 -0.00508978]\n",
      "   [-0.00916727 -0.00657574]\n",
      "   ..., \n",
      "   [-0.01346431  0.00196689]\n",
      "   [-0.01339939 -0.00136221]\n",
      "   [-0.01804661 -0.00132351]]]]\n",
      "0"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <type 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6786074c8255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# stop our queue threads and properly close the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1105\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \"\"\"\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[0;32m--> 231\u001b[0;31m                       (fetch, type(fetch)))\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <type 'NoneType'>"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "    # initialize the variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    print \"from the train set:\"\n",
    "    images, labels = iterator.get_next()\n",
    "    for i in range(12):\n",
    "        print sess.run(training_step(i, images.eval(), labels.eval()))\n",
    "\n",
    "    # stop our queue threads and properly close the session\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
